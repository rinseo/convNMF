USV <- torch_svd(torch_target)
# Truncate the SVD and normalize the singular values
U <- USV[[1]][ ,1:wavelet_rank] # 2D array
S <- USV[[2]][  1:wavelet_rank] # 1D array
V <- USV[[3]][ ,1:wavelet_rank] # 2D array
# Truncate, weight, and transpose the factors as appropriate
torch_footprint <- U
torch_profile <- torch_matmul(torch_diag(S) / torch_norm(S),
V$transpose(1,2))
# Revert back to array
new_footprint <- as.array(ensure3D(torch_footprint))
new_profile <- as.array(ensure3D(torch_profile))
return(list(new_footprint, new_profile))
}
map_estimate_fast <- function(wavelets, amplitudes, data,
noise_std=1.0, amp_rate=5.0, wavelet_rank=1,
num_iters=20, tol=1e-06, show_progress=TRUE){
# Fit the wavelets and amplitudes by maximum a posteriori (MAP) estimation
K <- dim(wavelets)[1]
N <- dim(wavelets)[2]
D <- dim(wavelets)[3]
Ts <- dim(data)[2]
# Initialize the residual
torch_amplitudes <- torch_tensor(amplitudes)
torch_wavelets <- ensure3D(torch_tensor(wavelets))
torch_wavelets_flip <- torch_flip(torch_wavelets, dims=-1)
torch_pred <- torch_conv1d(torch_amplitudes,
torch_wavelets_flip$permute(c(2,1,3)),
padding=D-1)[,1:Ts]
pred <- as.array(torch_pred) # convert back to array
residual <- data - pred
# Initialize the wavelet factors
stopifnot(wavelet_rank >= 1)
USV <- torch_svd(torch_wavelets)
U <- USV[[1]][,,1:wavelet_rank] # 3D array
S <- USV[[2]][ ,1:wavelet_rank] # 2D array
V <- USV[[3]][,,1:wavelet_rank] # 3D array
# Define footprints/profiles
torch_footprints <- U
torch_profiles <- V * torch_unsqueeze(S, 2)
torch_profiles <- torch_profiles$permute(c(1,3,2))
# Revert back to array
footprints <- as.array(ensure3D(torch_footprints))
profiles <- as.array(ensure3D(torch_profiles))
# Track log likelihoods over iterations
lls <- log_likelihood(wavelets, amplitudes, data, noise_std)
# Coordinate ascent
if (show_progress){
pb <- progress_bar$new(
total = num_iters,
format = "iter :current/:total [:bar] :percent eta: :eta")
}
for (iter in 1:num_iters){
# Update neurons one at a time
for (k in 1:K){
# Update the residual in place (add a_k \circledast W_k)
residual <- update_residual(
k, footprints, profiles, amplitudes, residual)
# Update the wavelet and amplitude with the residual
amplitudes[k,] <- update_amplitude_fast(
k, footprints, profiles, amplitudes, residual, noise_std, amp_rate)
new_wavelets <- update_wavelet_factors(
k, footprints, profiles, amplitudes, residual, wavelet_rank)
footprints[k,,] <- new_wavelets[[1]]
profiles[k,,] <- new_wavelets[[2]]
# Downdate the residual in place (subtract a_k \circledast W_k)
residual <- downdate_residual(
k, footprints, profiles, amplitudes, residual)
}
# Reconstruct the wavelets in place
torch_footprints <- ensure3D(torch_tensor(footprints))
torch_profiles <- ensure3D(torch_tensor(profiles))
torch_wavelets <- torch_matmul(torch_footprints, torch_profiles)
wavelets <- as.array(ensure3D(torch_wavelets))
# Compute the log likelihood
lls <- append(lls, log_likelihood(wavelets, amplitudes, data, noise_std))
# Check for convergence
if (abs(lls[iter+1] - lls[iter]) < tol){
break
}
# Step the progress bar
if (show_progress){
pb$tick()
}
}
# Check for convergence and warn if necessary
if (abs(lls[iter+1] - lls[iter]) < tol){
cat(sprintf("Iteration [%d/%d] Convergence detected!\n",iter,num_iters))
} else{
warning(sprintf("Increase num_iters > %d",num_iters))
}
return(list(wavelets=wavelets, amplitudes=amplitudes, lls=lls))
}
#---------------#
# Main Function #
#---------------#
#' Fit ConvNMF Model
#'
#' Fits a convolutional NMF model to multi-channel time-series data using MAP estimation.
#'
#' @param data Matrix of dimensions \code{num_channels} Ã— \code{num_samples}.
#' @param sample_freq Sampling frequency (Hz).
#' @param num_neurons Number of neurons to be fitted.
#' @param len_waveletf Length of spatial wavelet (in samples).
#' @param mean_spikes Expected number of spikes per unit time.
#' @param mean_amplitude Mean of the Gamma distribution for amplitudes.
#' @param shape_amplitude Shape of the Gamma distribution for amplitudes.
#' @param noise_std Standard deviation of Gaussian noise.
#' @param amp_rate Detection threshold in SD units.
#' @param wavelet_rank SVD rank for wavelet decomposition.
#' @param num_iters Maximum number of MAP iterations.
#' @param tol Convergence tolerance.
#'
#' @return A fitted \code{\link{ConvNMF-class}} object.
#' @export
convNMF <- function(data, # multi-channel time-series array (NxT)
sample_freq, # number of samples per unit time
num_neurons, # number of neurons
len_wavelet, # length of spikes
mean_spikes=10, # expected number of spikes per unit time
mean_amplitude=15, # alpha/beta of Gamma (alpha, beta)
shape_amplitude=3, # alpha of Gamma (alpha, beta)
noise_std=1, # MNV(0, noise_std^2)
amp_rate=3, # detect amplitudes (amp_rate) times higher than SD
wavelet_rank=1, # Dimension reduction in SVD
num_iters=50, # number of iterations
tol=1e-06, # tolerance level
show_progress=TRUE # show progress bar
){
# Get data dimensions
num_channels <- dim(data)[1] # number of channels, N
num_samples <- dim(data)[2] # number of samples, T
cat(paste0("Using (N x T) = (",num_channels," x ",num_samples,") data.\n"))
data <- array(data, dim=c(num_channels, num_samples))
# Make random wavelets
cat(paste0("Generating (K x N x D) = (",num_neurons," x ",num_channels," x ",len_wavelet,") wavelets.\n"))
wavelets <- generate_wavelets(num_neurons, num_channels, len_wavelet)@value
# Make zero amplitudes
cat(paste0("Initializing (K x T) = (",num_neurons," x ",num_samples,") amplitudes.\n"))
amplitudes <- array(0, dim=c(num_neurons, num_samples))
# Fit the method
est <- map_estimate_fast(wavelets, amplitudes, data,
noise_std, amp_rate, wavelet_rank, num_iters, tol, show_progress)
# Reorder by spiking times
times <- apply(est$wavelets, 1, function(x) which(abs(x)==max(abs(x)),arr.ind=T)[2])
wavelets_r <- array(est$wavelets[order(times),,,drop=FALSE],
dim=c(num_neurons, num_channels, len_wavelet))
amplitudes_r <- array(est$amplitudes[order(times),],
dim=c(num_neurons, num_samples))
# Re-normalize wavelets
w_kn  <- apply(apply(wavelets_r, c(1,2), max), 2, sum)     + 1e-06
w_k   <- apply(wavelets_r, 1, function(x) quantile(x,.95)) + 1e-06
w_knd <- array(w_k %*% t(w_kn), dim(wavelets_r))
wavelets_r_knd <- wavelets_r / w_knd
weights <- w_knd * apply(wavelets_r_knd, 1, norm, "2")
wavelets_r <- wavelets_r / weights
# Re-order channels
new_channels <- unlist(apply(wavelets_r, 1, # per neuron
function(x) which(apply(x,1,max) > .1))) # channel max > .1
new_channels <- unique(c(new_channels, 1:dim(wavelets_r)[2]))
wavelets_r <- wavelets_r[,rev(new_channels),]
weights_r <- weights[,rev(new_channels),]
data <- data[rev(new_channels),]
# Create ConvNMF object
est.ConvNMF <- new("ConvNMF",
data=new("Data",
value=data,
num_channels=num_channels, # N
num_samples=num_samples), # T
wavelets=new("Wavelets",
value=wavelets_r,
num_neurons=num_neurons, # K
num_channels=num_channels, # N
len_wavelet=len_wavelet, # D
weights=weights_r),
amplitudes=new("Amplitudes",
value=amplitudes_r,
num_neurons=num_neurons, # K
num_samples=num_samples), # T
lls=est$lls,
fit_args = list(
data=data,
sample_freq=sample_freq,
num_neurons=num_neurons,
len_wavelet=len_wavelet,
mean_spikes=mean_spikes,
mean_amplitude=mean_amplitude,
shape_amplitude=shape_amplitude,
noise_std=noise_std,
amp_rate=amp_rate,
wavelet_rank=wavelet_rank,
num_iters=num_iters,
tol=tol,
show_progress=show_progress
)
)
return(est.ConvNMF)
}
plot(predict(est))
plot(screen_data)+labs(title="Reference")+
plot(predict(est))+labs(title="Prediction")
sprintf("logLik = %.2f",logLik(est))
sprintf("%% power explained = %.2f",power_explained(est))
# sprintf("Sequenciness = %.2f",sequenciness(est))
# extract_spikes(est)
# hist(screen_data@value - predict(est)@value)
set.seed(1)
est <- convNMF(
data=screen_data@value, # multi-channel time-series array (NxT)
sample_freq=60, # number of samples per unit time
num_neurons=4, # number of neurons
len_wavelet=10, # length of spikes
mean_spikes=6, # expected number of spikes per unit time
mean_amplitude=15, # alpha/beta of Gamma (alpha, beta)
shape_amplitude=3, # alpha of Gamma (alpha, beta)
noise_std=1, # MVN(0, noise_std^2)
amp_rate=5, # detect amplitudes (amp_rate) times higher than SD
wavelet_rank=1, # dimension reduction in SVD
num_iters=50, # number of iterations
tol=1e-06 # tolerance level
)
plot(est)
# plot(est@amplitudes)
# plot(est@wavelets)
# est2 <- est
# W <- est@wavelets@value
#
# apply(apply(W[1,,],2,max),2,sum)
#
# which(apply(W[1,,],1,max)>.1)
#
# wavelet_set <- unlist(apply(W, 1, function(x) which(apply(x,1,max)>.1)))
# wavelet_set <- unique(c(wavelet_set, 1:dim(W)[2]))
#
# W2 <- W[,rev(wavelet_set),]
# est2@wavelets@value <- W2
#
# norm(W[1,,],"2")
#
# # Wmax <- array(apply(W, c(1,2), sum),dim=dim(W))
# # W2 <- W / Wmax
#
# # Wk_sum1 <- apply(apply(W, c(1,2), max), 1, sum)
# Wk_sum1 <- apply(apply(W, c(1,2), max), 2, sum) + 1e-06
# Wk_sum2 <- apply(W, 1, function(x) quantile(x,.95)) + 1e-06
# W2 <- W/(array(Wk_sum2 %*% t(Wk_sum1),dim=dim(W)))
# W2 <- W2 / apply(W2, 1, norm, "2")
#
# # W2 <- W * ifelse(W>=.1 & W<.5, 1.5, 1)
# # W2 <- W / Wmax
# # W2 <- W / Wk_sum1
# est2@wavelets@value <- W2
# plot(est2@wavelets)
# plot(est2)
# plot(est@lls, type="b", xlab="Iteration", ylab="Normalized Log Likelihood")
plot(predict(est))
plot(screen_data)+labs(title="Reference")+
plot(predict(est))+labs(title="Prediction")
sprintf("logLik = %.2f",logLik(est))
sprintf("%% power explained = %.2f",power_explained(est))
# sprintf("Sequenciness = %.2f",sequenciness(est))
# extract_spikes(est)
# hist(screen_data@value - predict(est)@value)
knitr::opts_chunk$set(message = TRUE, warning = FALSE)
wd <- "~/Library/CloudStorage/OneDrive-Stanford/2223_Quarter4/NeuralScreen/NSP Rpackage/convNMF/"
devtools::build(wd)
devtools::document(wd)
devtools::install(wd)
set.seed(1)
init <- generate(
num_channels=5, # number of channels
num_samples=1000, # number of samples
sample_freq=1000, # number of samples per unit time
num_neurons=2, # number of neurons
len_wavelet=10, # length of spikes
mean_spikes=10, # expected number of spikes per unit time
mean_amplitude=15, # alpha/beta of Gamma (alpha, beta)
shape_amplitude=3, # alpha of Gamma (alpha, beta)
noise_std=1 # MVN(0, noise_std^2)
)
plot(init)
est <- convNMF(
data=init@data@value, # multi-channel time-series array (NxT)
sample_freq=1000, # number of samples per unit time
num_neurons=2, # number of neurons
len_wavelet=10, # length of spikes
mean_spikes=10, # expected number of spikes per unit time
mean_amplitude=15, # alpha/beta of Gamma (alpha, beta)
shape_amplitude=3, # alpha of Gamma (alpha, beta)
noise_std=1, # MVN(0, noise_std^2)
amp_rate=5, # detect amplitudes (amp_rate) times higher than SD
wavelet_rank=1, # dimension reduction in SVD
num_iters=50, # number of iterations
tol=1e-04 # tolerance level
)
plot(est)
plot(init@data)
plot(init@amplitudes)
plot(init@wavelets)
init@wavelets
init@amplitudes
plot(init@amplitudes)
knitr::opts_chunk$set(message = TRUE, warning = FALSE)
wd <- "/Users/rinseo/Library/CloudStorage/OneDrive-Stanford/2223_Quarter4/NeuralScreen"
knitr::opts_knit$set(root.dir = wd)
library(convNMF)
library(tidyverse) # for data manipulation
library(progress) # for progress check
library(patchwork) # for data viz
##
library(VGAM) # rzipois
channels <- c("BROWSER","BUSINESS","EVENTS","GAME","IM","LAUNCHER",
"MAPS_AND_NAVIGATION","PHONE_AND_SMS","PHOTOGRAPHY","SHOPPING",
"SOCIAL","STOCK","SYSTEM","TOOLS","VIDEO_PLAYERS",
"OTHERS","SCREEN_OFF")
# channels <- c("BROWSER","BUSINESS","EVENTS","GAME","IM","LAUNCHER",
#               "MAPS_AND_NAVIGATION","PHONE_AND_SMS","PHOTOGRAPHY","SHOPPING",
#               "SOCIAL","STOCK","SYSTEM","TOOLS","VIDEO_PLAYERS",
#               "OTHERS")
# channels <- c("IM","LAUNCHER","VIDEO_PLAYERS","SOCIAL")
# fulldata <- read.csv("image_sensor_combined.csv")
# # create 17 app categories
# mydata <- fulldata %>%
#   mutate(record_time=str_extract(filename,"\\d+$") %>% ymd_hms(),
#          channel=ifelse(str_detect(fg_app_category,"GAME"),"GAME",fg_app_category),
#          channel=toupper(channel),
#          channel=ifelse(channel %in% channels, channel, "OTHERS"),
#          channel=ifelse(screen_status=="Screen_off","SCREEN_OFF",channel)) %>%
#   select(PID,record_time,kill_time,is_available,screen_status,
#          fg_app,fg_app_category,channel,sex:employee)
# write_excel_csv(mydata,"image_sensor_combined_selected.csv") ## BIG!
# # aggregated every minute
# mydata_mn <- mydata %>%
#   mutate(time = make_datetime(year=year(record_time),
#                               month=month(record_time),
#                               day=day(record_time),
#                               hour=hour(record_time),
#                               min=minute(record_time)),
#          n_channel=factor(channel,levels=channels,labels=1:17)) %>%
#   group_by(PID,time,channel,n_channel) %>%
#   count(channel) %>%
#   group_by(PID,time) %>%
#   mutate(n_time=sum(n)) %>% # how many obs per min?
#   ungroup() %>%
#   mutate(time_spent=ifelse(n_time<=12,5*n, # every 5 seconds
#                            n/n_time*60 # more freq. so proportion
#                            ))
# write_excel_csv(mydata_mn,"image_sensor_combined_selected_perMin.csv")
# load minute-level data
mydata_mn <- read_csv("image_sensor_combined_selected_perMin.csv")
# make a person-specific subset
mydata_mn %>% count(PID)
mydata_mn_sub <- mydata_mn %>% filter(PID %in% c("P47"))
# mydata_mn_sub <- mydata_mn %>% filter(PID %in% c("P24"))
mydata_mn_sub_wide <- mydata_mn_sub %>%
select(PID,time,channel,time_spent) %>%
pivot_wider(names_from=channel,values_from=time_spent,values_fill=0) %>%
select(PID,time,all_of(channels)) %>%
mutate(time=as_datetime(time)) %>%
arrange(PID,time)
# fill missing time intervals
mydata_mn_sub_time <- seq(min(mydata_mn_sub_wide$time),
max(mydata_mn_sub_wide$time),
by="1 min")
mydata_mn_sub_wide_full <- mydata_mn_sub_wide %>%
right_join(expand.grid(PID=unique(mydata_mn_sub_wide$PID),
time=mydata_mn_sub_time),by=c("PID","time")) %>%
arrange(PID,time) %>%
mutate(across(all_of(channels),~ifelse(is.na(.),0,.)))
# example day 1
df <- mydata_mn_sub_wide_full %>%
filter(between(time,
as_datetime("2021/10/22 08:00:00"), # start_time
as_datetime("2021/10/22 22:00:00")) # end_time
)
# df <- mydata_mn_sub_wide_full %>%
#   filter(between(time,
#                  as_datetime("2020/12/11 08:00:00"), # start_time
#                  as_datetime("2020/12/11 22:00:00")) # end_time
#          )
# df <- mydata_mn_sub_wide_full
df_values <- df[channels] %>% as.matrix()
# Intercept shifts
df_values_nn <- df_values[rowSums(df_values)>0,] # only non-zero elements
# df_values_nn <- df_values
df_values_nn_Means <- colMeans(df_values_nn)
# df_values_nn_Means <- apply(df_values_nn,2,function(x) quantile(x,.975))
df_values_nn_Means
# Re-scaling variances
df_values_nn_SDs <- apply(df_values_nn, 2, sd)
df_values_nn_SDs
# slice again
# df <- df %>%
#   filter(PID %in% c("P47")) %>%
#   filter(between(time,
#                  as_datetime("2021/10/22 08:00:00"), # start_time
#                  as_datetime("2021/10/22 22:00:00")) # end_time
#          )
# df_values <- df[channels] %>% as.matrix()
# hist(log(df_values))
# Standardize values
# X <- (t(df_values) - df_values_nn_Means) / df_values_nn_SDs # N x Ts
# hist(X)
# X <- t(log(df_values+1))
# hist(X)
# X <- t(df_values) / df_values_nn_SDs
X <- (t(df_values) - df_values_nn_Means)
X <- t(df_values)
# # Re-normalize wavelets
# w_kn  <- apply(apply(wavelets_r, c(1,2), max), 2, sum)     + 1e-06
# w_k   <- apply(wavelets_r, 1, function(x) quantile(x,.95)) + 1e-06
# w_knd <- array(w_k %*% t(w_kn), dim(wavelets_r))
# wavelets_r_knd <- wavelets_r / w_knd
# weights <- w_knd * apply(wavelets_r_knd, 1, norm, "2")
# wavelets_r <- wavelets_r / weights
# X <- X - min(X)
# X <- X + 30
# X <- t(df_values)
# X <- t(log(df_values+1))
# X <- (t(df_values) - apply(df_values,2,min))/(apply(df_values,2,max)-apply(df_values,2,min))
# hist(X)
# df_values_nn_95s <- apply(df_values_nn,2,function(x) quantile(x,.95))
# df_values_nn_95s
# X <- t(df_values) / df_values_nn_95s
# Impute negative values with zeros
X[X<0] <- 0
# X[is.na(X)] <- 0
# rowMeans(X) # Compute the means
# sd(X) # Compute the sd
# min(X)
# max(X)
# X <- X / sd(X)
sd(X)
# apply(df_values_nn, 2, max)
# apply(df_values_nn, 2, min)
X_mds <- cmdscale(dist(X), k=1) %>%
enframe(name="channel",value="coord") %>%
arrange(coord)
X_mds # 1d coordinates
# X <- X[X_mds$channel,]
rownames(X)
print(dim(X))
screen_data <- new("Data",
value=X,
num_channels=dim(X)[1],
num_samples=dim(X)[2])
# plot(screen_data)
state_session <- read_csv("phone_usage_session_include_state.csv")
state_session_sub <- state_session %>%
filter(PID %in% unique(df$PID),
date %in% unique(date(df$time)))
state_session_sub
kill_session <- state_session_sub %>% filter(kill_time==1)
avil_session <- state_session_sub %>% filter(is_available==1)
set.seed(1)
est <- convNMF(
data=screen_data@value, # multi-channel time-series array (NxT)
sample_freq=60, # number of samples per unit time
num_neurons=4, # number of neurons
len_wavelet=10, # length of spikes
mean_spikes=6, # expected number of spikes per unit time
mean_amplitude=15, # alpha/beta of Gamma (alpha, beta)
shape_amplitude=3, # alpha of Gamma (alpha, beta)
noise_std=1, # MVN(0, noise_std^2)
amp_rate=5, # detect amplitudes (amp_rate) times higher than SD
wavelet_rank=1, # dimension reduction in SVD
num_iters=50, # number of iterations
tol=1e-06 # tolerance level
)
plot(est)
plot(est@wavelets)
est@wavelets
x <- est@wavelets
melt(x@value)
melt(x@value) %>% # Var1 (k) x Var2 (n) x Var3 (d) x value
mutate(value=Var2-1 + value) %>% # add n-1 to value (0,N)
ggplot(aes(x=Var3,y=value,color=as.factor(Var1), # colors specific to k
group=interaction(Var1,Var2)))+ # lines specific to k x n
geom_hline(yintercept=0:(x@num_channels-1),color="grey",linewidth=0.2)+
geom_path()+
# geom_smooth()+
# Add plot title per neuron
geom_rect(aes(fill=as.factor(Var1)),
xmin=-Inf,xmax=Inf,
ymin=x@num_channels,ymax=x@num_channels+1, # (N,N+1)
alpha=.5,linewidth=0)+
geom_text(aes(label=paste0("italic(W)[",Var1,"]"),
x=(1+x@len_wavelet)/2,y=x@num_channels+0.5),
color="black",parse=TRUE,
data=melt(x@value) %>% filter(Var2==1,Var3==1))+
facet_wrap(~Var1,nrow=1)+
geom_vline(xintercept=1,color="black", linewidth=1)+ # add border lines
# Adjust x/y scales
scale_x_continuous(limits=c(1,x@len_wavelet),
breaks=x@len_wavelet,
expand=c(0,0))+
scale_y_continuous(breaks=0:(x@num_channels-1), # break every integer (0,N)
labels=1:(x@num_channels),
limits=c(-0.5,x@num_channels+1), # (0,N+1)
expand=expansion(mult=c(0,0)))+
guides(color="none",fill="none")+
theme_classic()+
theme(axis.line.y=element_blank(), # added y border lines manually
strip.background = element_blank(),
strip.text = element_blank(),
panel.spacing.x = unit(0.2, "lines"))+ # spacing between neurons
labs(x=parse(text="Delay~italic(d)"),
y=parse(text="Channels~italic(n)"))
knitr::opts_chunk$set(message = TRUE, warning = FALSE)
wd <- "~/Library/CloudStorage/OneDrive-Stanford/2223_Quarter4/NeuralScreen/NSP Rpackage/convNMF/"
devtools::build(wd)
devtools::document(wd)
